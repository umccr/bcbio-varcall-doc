import os
import glob
import re
import socket
import gzip
import csv
import pprint
from collections import defaultdict

""" 
Usage: 
$ snakemake -p --configfile=config.yaml
"""

# configfile: 'config.yaml'
# shell.prefix("set -o pipefail; ")

hostname = socket.gethostname()
if hostname == '5180L-135800-M.local':
    loc = 'local'
    ref_loc = '/Users/vsaveliev/genomes/Hsapiens'
elif re.match(r'spartan.*\.hpc\.unimelb\.edu\.au', hostname):
    loc = 'spartan'
    ref_loc = '/home/vlad/bcbio/genomes/Hsapiens'

# vcf_dir = '/Users/vsaveliev/Analysis/snv_validation/dream_syn3_grch37'
# fn_by_sample = {
#     re.sub('.vcf.gz$', '', fn.replace('-annotated', '')): os.path.join(vcf_dir, fn) 
#     for fn in glob_wildcards(join(vcf_dir, '*.vcf.gz'))
# }
# sample_name = 'syn3-tumor'

rule all:
    input:
        'work/eval/report.tsv'
    output:
        'report.tsv'
    shell:
        'ln -sr {input} {output}'

rule prep_regions:
    input:
        truth_regions = os.path.join(ref_loc, config['truth_regions'])
    output:
        'work/regions/regions.bed'
    run:
        samples_regions = config.get('regions')
        if samples_regions:
            shell('bedops -i {input.truth_regions} {samples_regions} > {output}')
        else:
            shell('ln -sr {input.truth_regions} {output}')

rule narrow_samples_to_target:  # Extracts target sample, target regions, and remove rejected calls from the input VCF 
    input:
        lambda wildcards: config['samples'][wildcards.sample]['vcf_file'],
        regions = rules.prep_regions.output
    output:
        'work/narrow/{sample}.vcf.gz'
    params:
        vcf_sample = lambda wildcards: config['samples'][wildcards.sample]['vcf_sample']
    shell:
        'bcftools view -s {params.vcf_sample} {input[0]} -T {input.regions} -f .,PASS -Oz -o {output[0]} && tabix -p vcf {output[0]}'

rule narrow_truth_to_target:  # Extracts target regions from truth VCF
    input:
        truth_variants = os.path.join(ref_loc, config['truth_variants']),
        regions = rules.prep_regions.output
    output:
        'work/narrow/truth_variants.vcf.gz'
    shell:
        'bcftools view {input.truth_variants} -T {input.regions} -Oz -o {output[0]} && tabix -p vcf {output[0]}'

# def decompose_and_normalize(vcf, output, reference_fasta=None):
    # return ("gunzip -c {vcf} | " +
    #     "sed 's/ID=AD,Number=./ID=AD,Number=R/' | " +
    #     "vt decompose -s - " +
    #     "| vt normalize -n -r {reference_fasta} - " +
    #     "| awk '{{ gsub(\"./-65\", \"./.\"); print $0 }}'" +
    #     "| sed -e 's/Number=A/Number=1/g' | bgzip -c > {output}").format(**locals())

normalize = "bcftools norm -m '-' {input.vcf} -Oz -o {output[0]}"
reference_fasta = os.path.join(ref_loc, config['reference_fasta'])
if os.path.isfile(reference_fasta):
    normalize += ' -f ' + reference_fasta
normalize += ' && tabix -p vcf {output[0]}'

rule normalize_sample:
    input:
        vcf = rules.narrow_samples_to_target.output[0]
    output:
        'work/normalize/{sample}/{sample}.vcf.gz'
    shell:
        normalize

rule normalize_truth:
    input:
        vcf = rules.narrow_truth_to_target.output[0]
    output:
        'work/normalize/truth_variants.vcf.gz'
    shell:
        normalize

rule bcftools_isec:
    input:
        sample_vcf = rules.normalize_sample.output,
        truth_vcf = rules.normalize_truth.output,
        regions = rules.prep_regions.output
    params:
        output_dir = 'work/eval/{sample}_bcftools_isec'
    output:
        fp = 'work/eval/{sample}_bcftools_isec/0000.vcf',
        fn = 'work/eval/{sample}_bcftools_isec/0001.vcf',
        tp = 'work/eval/{sample}_bcftools_isec/0002.vcf'
    shell:
        'bcftools isec {input.sample_vcf} {input.truth_vcf} -T {input.regions} -p {params.output_dir}'

def count_variants(vcf):
    snps = 0
    indels = 0
    with (gzip.open(vcf) if vcf.endswith('.gz') else open(vcf)) as f:    
        for l in [l for l in f if not l.startswith('#')]:
            _, _, _, ref, alt = l.split('\t')[:5]
            if len(ref) == len(alt) == 1:
                snps += 1
            else:
                indels += 1
    return snps, indels

rule eval:
    input: 
        fp = rules.bcftools_isec.output.fp,
        fn = rules.bcftools_isec.output.fn,
        tp = rules.bcftools_isec.output.tp
    output:
        'work/eval/{sample}_stats.tsv'
    run:
        fp_snps, fp_inds = count_variants(input.fp)
        fn_snps, fn_inds = count_variants(input.fn)
        tp_snps, tp_inds = count_variants(input.tp)

        with open(output[0], 'w') as f:
            writer = csv.writer(f, delimiter='\t')
            writer.writerow([
                '#SNP TP', 'SNP FP', 'SNP FN', 'SNP Precision', 'SNP Recall', 
                 'IND TP', 'IND FP', 'IND FN', 'IND Precision', 'IND Recall'
            ])
            writer.writerow([
                tp_snps, fp_snps, fn_snps, tp_snps / (tp_snps + fp_snps), tp_snps / (tp_snps + fn_snps),
                tp_inds, fp_inds, fn_inds, tp_inds / (tp_inds + fp_inds), tp_inds / (tp_inds + fn_inds)
            ])

# rule eval:
#     input:
#         rules.index_samples.output,
#         rules.index_truth.output,
#         regions = rules.prep_target.output,
#         truth_vcf = rules.narrow_truth_to_target.output, 
#         sample_vcf = rules.narrow_samples_to_target.output
#     output:
#         '{sample}/{sample}.re.a/weighted_roc.tsv.gz'
#     shell:
#         '{rtgeval}/run-eval -s {sdf}'
#         ' -b {input.regions}'
#         ' {input.truth_vcf}'
#         ' {input.sample_vcf}'

rule report:
    input:
        stats_files = expand(rules.eval.output, sample=sorted(config['samples'].keys()))
    output:
        'work/eval/report.tsv'
    params:
        samples = sorted(config['samples'].keys())
    run:
        out_lines = []
        out_lines.append(['Sample', 'File', 'SNP', ''  , ''  , ''         , ''      , 'INDEL', ''  , ''  , ''         , ''       ])
        out_lines.append([''      , ''    , 'TP' , 'FP', 'FN', 'Precision', 'Recall', 'TP'   , 'FP', 'FN', 'Precision', 'Recall',])
        for stats_file, sname in zip(input.stats_files, params.samples):
            print(stats_file, sname)
            with open(stats_file) as f:
                out_lines.append([sname, stats_file] + f.readlines()[1].strip().split('\t'))

        with open(output[0], 'w') as out_f:
            for fields in out_lines:
                print(fields)
                out_f.write('\t'.join(map(str, fields)) + '\n')

# rule count_truth:
#     input:
#         truth_variants
#     output:
#         snps = 'truth.snps',
#         indels = 'truth.indels'
#     run:
#         snps, indels = count_variants(truth_variants)
#         with open(output.snps, 'w') as o:
#             o.write(snps)
#         with open(output.indels, 'w') as o:
#             o.write(indels)

# rule report:
#     input:
#         stats_files = expand(rules.eval.output, sample=config['samples'].keys()),
#         truth_snps = rules.count_truth.output.snps,
#         truth_indels = rules.count_truth.output.indels
#     output:
#         'report.tsv'
#     params:
#         samples = config['samples']
#     run:
#         truth_snps = int(open(input.truth_snps).read())
#         truth_indels = int(open(input.truth_indels).read())

#         out_lines = []
#         out_lines.append(['', 'SNP', ''  , ''  , 'INDEL', ''  , ''  ])
#         out_lines.append(['', 'TP' , 'FP', 'FN', 'TP'   , 'FP', 'FN'])

#         for stats_file, sname in zip(input.stats_files, params.samples):
#             data = defaultdict(dict)
#             with open(stats_file) as f:
#                 for l in f:
#                     if l:
#                         event_type, change_type, metric, val = l.strip().split()[:4]
#                         if event_type == 'allelic':
#                             try:
#                                 val = int(val)
#                             except ValueError:
#                                 val = float(val)
#                             data[change_type][metric] = val
#             pprint.pprint(data)
#             try:
#                 out_lines.append([sname, truth_snps   - data['SNP']['FN'],   data['SNP']['FP'],   data['SNP']['FN'], 
#                                          truth_indels - data['INDEL']['FN'], data['INDEL']['FP'], data['INDEL']['FN']])
#             except KeyError:
#                 print('Some of the required data for ' + sname + ' not found in ' + fp)

#         with open(output[0], 'w') as out_f:
#             for fields in out_lines:
#                 print(fields)
#                 out_f.write('\t'.join(map(str, fields)) + '\n')


# ensemble.subsample.re.truth.vcf.gz      Truth = 396 = TP + FN
# ensemble.subsample.re.test.vcf.gz       Call  = 366 + TP + FP
# fp.vcf.gz                               FP    = 19
# fn.vcf.gz                               FN    = 49
# tp.vcf.gz                               TP    = 347
# bcftools_isec/0000.vcf.gz               FP    = 19     for records private to  ensemble.subsample.re.test.vcf.gz
# bcftools_isec/0001.vcf.gz               FN    = 49     for records private to  ensemble.subsample.re.truth.vcf.gz
# bcftools_isec/0002.vcf.gz               TP    = 347    for records from ensemble.subsample.re.test.vcf.gz shared by both       ensemble.subsample.re.test.vcf.gz ensemble.subsample.re.truth.vcf.gz


# bcftools_isec/0000.vcf.gz               FP    = 71     for records private to  ensemble.subsample.re.test.vcf.gz












